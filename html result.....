from bs4 import BeautifulSoup
import re
import os
import json

def extract_strengths_weaknesses(html_file_path):
    """Extract strengths and weaknesses from a single HTML file."""
    with open(html_file_path, 'r', encoding='utf-8') as file:
        soup = BeautifulSoup(file.read(), 'html.parser')
    
    return extract_from_soup(soup)

def clean_text(text):
    """Clean and normalize text content."""
    if not text:
        return ""
    
    # Remove extra whitespace and normalize
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    # Remove leading/trailing colons
    text = text.strip(':').strip()
    
    return text

def extract_from_soup(soup):
    """Extract strengths and weaknesses with comprehensive debugging."""
    
    # Get the full text content
    full_text = soup.get_text()
    
    # Clean up the text
    full_text = re.sub(r'\s+', ' ', full_text)
    
    print(f"ğŸ“„ Total document length: {len(full_text)} characters")
    
    # STEP 1: Find the target section containing Key Rating Drivers
    print("\nğŸ” STEP 1: Finding Key Rating Drivers section...")
    
    target_text = find_key_rating_section(full_text)
    
    if not target_text:
        print("âŒ Could not find Key Rating Drivers section")
        return {}, {}
    
    print(f"âœ… Found target section: {len(target_text)} characters")
    
    # STEP 2: Show the raw content for manual inspection
    print("\nğŸ“„ STEP 2: Raw target content:")
    print("=" * 80)
    print(target_text[:1000])  # Show first 1000 characters
    if len(target_text) > 1000:
        print(f"\n... (showing first 1000 of {len(target_text)} characters)")
    print("=" * 80)
    
    # STEP 3: Find Strengths and Weaknesses with multiple approaches
    print("\nğŸ” STEP 3: Extracting Strengths and Weaknesses...")
    
    strengths_dict, weaknesses_dict = extract_sections_smart(target_text)
    
    return strengths_dict, weaknesses_dict

def find_key_rating_section(full_text):
    """Find the Key Rating Drivers section with multiple approaches."""
    
    # Approach 1: Look for explicit "Key Rating Drivers" section
    patterns = [
        r'Key Rating Drivers[^a-zA-Z]*(?:Detailed Description)?\s*(.*?)(?=Liquidity\s*:|Outlook\s*:|Analytical Approach|About the Company|Rating sensitivity)',
        r'Key Rating Drivers.*?:\s*(.*?)(?=Liquidity|Outlook|Analytical|About)',
        r'(?:Key Rating Drivers|Detailed Description)(.*?)(?=Liquidity|Outlook|Analytical)'
    ]
    
    for i, pattern in enumerate(patterns, 1):
        match = re.search(pattern, full_text, re.DOTALL | re.IGNORECASE)
        if match:
            content = match.group(1).strip()
            if len(content) > 100:  # Must be substantial
                print(f"âœ… Found section using pattern {i}")
                return content
    
    # Approach 2: Look for sections that contain both "Strengths" and "Weakness"
    print("ğŸ”„ Trying fallback approach...")
    
    strengths_pos = full_text.lower().find('strengths')
    weakness_pos = full_text.lower().find('weakness')
    
    if strengths_pos > -1 and weakness_pos > -1:
        # Find reasonable boundaries
        start_pos = max(0, strengths_pos - 200)
        
        # Look for end markers
        end_markers = ['liquidity', 'outlook', 'analytical approach', 'about the company']
        end_pos = len(full_text)
        
        for marker in end_markers:
            marker_pos = full_text.lower().find(marker, weakness_pos)
            if marker_pos > -1 and marker_pos < end_pos:
                end_pos = marker_pos
        
        content = full_text[start_pos:end_pos].strip()
        print(f"âœ… Found section using fallback (positions: {start_pos}-{end_pos})")
        return content
    
    return None

def extract_sections_smart(target_text):
    """Smart extraction with multiple methods and detailed debugging."""
    
    strengths_dict = {}
    weaknesses_dict = {}
    
    print("\nğŸ” Analyzing text structure...")
    
    # Check what we have
    has_strengths = 'strengths' in target_text.lower()
    has_weakness = 'weakness' in target_text.lower()
    
    print(f"   ğŸ“Š Contains 'Strengths': {has_strengths}")
    print(f"   ğŸ“Š Contains 'Weakness': {has_weakness}")
    
    if not has_strengths and not has_weakness:
        print("âŒ No strength/weakness keywords found")
        return strengths_dict, weaknesses_dict
    
    # METHOD 1: Try precise regex extraction
    print("\nğŸ”„ METHOD 1: Regex-based extraction...")
    
    if has_strengths:
        strengths_patterns = [
            r'Strengths?\s*:?\s*(.*?)(?=Weakness|Liquidity|Outlook|$)',
            r'Strengths?\s*:?\s*(.*?)(?=Weakness)',
            r'Strengths?\s*:\s*(.*?)(?=\n\s*Weakness|\n\s*Liquidity)',
            r'Strengths?\s*(?::|\.)\s*(.*?)(?=Weakness|Liquidity|Outlook)'
        ]
        
        for i, pattern in enumerate(strengths_patterns, 1):
            match = re.search(pattern, target_text, re.DOTALL | re.IGNORECASE)
            if match:
                strengths_text = match.group(1).strip()
                if len(strengths_text) > 20:
                    print(f"   âœ… Strengths found with pattern {i}: {len(strengths_text)} chars")
                    print(f"   ğŸ“„ Preview: {strengths_text[:150]}...")
                    strengths_dict = parse_content_to_dict(strengths_text, "Strengths")
                    break
        
        if not strengths_dict:
            print("   âŒ No strengths extracted with regex")
    
    if has_weakness:
        weakness_patterns = [
            r'Weakness(?:es)?\s*:?\s*(.*?)(?=Liquidity|Outlook|Analytical|$)',
            r'Weakness(?:es)?\s*:?\s*(.*?)(?=Liquidity|Outlook)',
            r'Weakness(?:es)?\s*:\s*(.*?)(?=\n\s*Liquidity|\n\s*Outlook)',
            r'Weakness(?:es)?\s*(?::|\.)\s*(.*?)(?=Liquidity|Outlook|Analytical)'
        ]
        
        for i, pattern in enumerate(weakness_patterns, 1):
            match = re.search(pattern, target_text, re.DOTALL | re.IGNORECASE)
            if match:
                weakness_text = match.group(1).strip()
                if len(weakness_text) > 20:
                    print(f"   âœ… Weaknesses found with pattern {i}: {len(weakness_text)} chars")
                    print(f"   ğŸ“„ Preview: {weakness_text[:150]}...")
                    weaknesses_dict = parse_content_to_dict(weakness_text, "Weaknesses")
                    break
        
        if not weaknesses_dict:
            print("   âŒ No weaknesses extracted with regex")
    
    # METHOD 2: Manual text splitting if regex fails
    if not strengths_dict and not weaknesses_dict:
        print("\nğŸ”„ METHOD 2: Manual text splitting...")
        
        lines = target_text.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Check for section headers
            if re.search(r'\bStrengths?\s*:?\s*$', line, re.IGNORECASE):
                if current_section and current_content:
                    # Process previous section
                    content_text = ' '.join(current_content)
                    if current_section == 'strengths':
                        strengths_dict = parse_content_to_dict(content_text, "Strengths")
                    elif current_section == 'weaknesses':
                        weaknesses_dict = parse_content_to_dict(content_text, "Weaknesses")
                
                current_section = 'strengths'
                current_content = []
                print(f"   ğŸ“ Found Strengths header: {line}")
                continue
            
            elif re.search(r'\bWeakness(?:es)?\s*:?\s*$', line, re.IGNORECASE):
                if current_section and current_content:
                    # Process previous section
                    content_text = ' '.join(current_content)
                    if current_section == 'strengths':
                        strengths_dict = parse_content_to_dict(content_text, "Strengths")
                
                current_section = 'weaknesses'
                current_content = []
                print(f"   ğŸ“ Found Weaknesses header: {line}")
                continue
            
            elif re.search(r'\b(?:Liquidity|Outlook|Analytical)\b', line, re.IGNORECASE):
                # End of our target content
                if current_section and current_content:
                    content_text = ' '.join(current_content)
                    if current_section == 'strengths':
                        strengths_dict = parse_content_to_dict(content_text, "Strengths")
                    elif current_section == 'weaknesses':
                        weaknesses_dict = parse_content_to_dict(content_text, "Weaknesses")
                break
            
            # Add content to current section
            if current_section:
                current_content.append(line)
        
        # Process final section
        if current_section and current_content:
            content_text = ' '.join(current_content)
            if current_section == 'strengths':
                strengths_dict = parse_content_to_dict(content_text, "Strengths")
            elif current_section == 'weaknesses':
                weaknesses_dict = parse_content_to_dict(content_text, "Weaknesses")
    
    # METHOD 3: Look for bullet points anywhere in the text
    if not strengths_dict and not weaknesses_dict:
        print("\nğŸ”„ METHOD 3: Looking for bullet patterns anywhere...")
        
        # Find all potential bullet points
        bullet_patterns = [
            r'(Strong [^:]{10,150}):\s*([^.]+(?:\.[^.]*)*)',
            r'(Robust [^:]{10,150}):\s*([^.]+(?:\.[^.]*)*)',
            r'(Exposure to [^:]{10,150}):\s*([^.]+(?:\.[^.]*)*)',
            r'(Well-[^:]{10,150}):\s*([^.]+(?:\.[^.]*)*)',
            r'([A-Z][a-z]+ [^:]{15,150}):\s*([^.]+(?:\.[^.]*){1,})'
        ]
        
        all_matches = []
        for pattern in bullet_patterns:
            matches = re.finditer(pattern, target_text, re.DOTALL)
            for match in matches:
                title = clean_text(match.group(1))
                description = clean_text(match.group(2))
                if len(title) > 5 and len(description) > 20:
                    all_matches.append((title, description, match.start()))
                    print(f"   ğŸ” Found potential item: {title[:50]}...")
        
        # Classify matches as strengths or weaknesses based on position and keywords
        strengths_keywords = ['strong', 'robust', 'well', 'leading', 'established', 'diversified']
        weakness_keywords = ['exposure', 'susceptible', 'dependent', 'vulnerable', 'limited']
        
        for title, description, position in all_matches:
            title_lower = title.lower()
            
            # Classify based on keywords
            is_strength = any(kw in title_lower for kw in strengths_keywords)
            is_weakness = any(kw in title_lower for kw in weakness_keywords)
            
            if is_strength and not is_weakness:
                strengths_dict[title] = description
            elif is_weakness and not is_strength:
                weaknesses_dict[title] = description
            else:
                # Classify based on position relative to "Strengths" and "Weakness" text
                strengths_pos = target_text.lower().find('strengths')
                weakness_pos = target_text.lower().find('weakness')
                
                if strengths_pos > -1 and weakness_pos > -1:
                    if abs(position - strengths_pos) < abs(position - weakness_pos):
                        strengths_dict[title] = description
                    else:
                        weaknesses_dict[title] = description
                elif strengths_pos > -1:
                    strengths_dict[title] = description
                elif weakness_pos > -1:
                    weaknesses_dict[title] = description
    
    print(f"\nğŸ“Š Final extraction results:")
    print(f"   ğŸ“ˆ Strengths: {len(strengths_dict)} items")
    print(f"   ğŸ“‰ Weaknesses: {len(weaknesses_dict)} items")
    
    return strengths_dict, weaknesses_dict

def parse_content_to_dict(content_text, section_name):
    """Parse content text into dictionary of title->description pairs."""
    
    print(f"\n   ğŸ” Parsing {section_name} content ({len(content_text)} chars)...")
    items = {}
    
    if len(content_text) < 20:
        print(f"   âš ï¸ Content too short")
        return items
    
    # Remove section header if it leaked in
    content_text = re.sub(r'^(?:Strengths?|Weakness(?:es)?)\s*:?\s*', '', content_text, flags=re.IGNORECASE)
    
    # Method 1: Look for colon-separated items
    colon_items = re.split(r'(?<=\.)\s+(?=[A-Z])', content_text)
    
    for item in colon_items:
        item = item.strip()
        if ':' in item and len(item) > 30:
            colon_pos = item.find(':')
            title = item[:colon_pos].strip()
            description = item[colon_pos+1:].strip()
            
            if (len(title) > 5 and len(title) < 300 and
                not title.lower().startswith(('the ', 'this ', 'it ', 'there '))):
                
                title = clean_text(title)
                description = clean_text(description)
                
                if title:
                    items[title] = description
                    print(f"      âœ… {title[:50]}...")
    
    # Method 2: If no colon items found, try pattern matching
    if not items:
        patterns = [
            r'(Strong [^:]{10,150}):\s*([^.]+(?:\.[^.]*)*)',
            r'(Robust [^:]{10,150}):\s*([^.]+(?:\.[^.]*)*)',
            r'(Exposure to [^:]{10,150}):\s*([^.]+(?:\.[^.]*)*)',
            r'([A-Z][^:]{15,150}):\s*([^.]+(?:\.[^.]*){1,})'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content_text, re.DOTALL)
            for match in matches:
                title = clean_text(match.group(1))
                description = clean_text(match.group(2))
                
                if title and len(title) > 5:
                    items[title] = description
                    print(f"      âœ… {title[:50]}...")
    
    print(f"   ğŸ“Š Parsed {len(items)} items from {section_name}")
    return items

def debug_html_structure(html_file_path):
    """Comprehensive debug function."""
    with open(html_file_path, 'r', encoding='utf-8') as file:
        soup = BeautifulSoup(file.read(), 'html.parser')
    
    print("ğŸ” COMPREHENSIVE HTML DEBUG")
    print("=" * 60)
    
    full_text = soup.get_text()
    full_text = re.sub(r'\s+', ' ', full_text)
    
    print(f"ğŸ“Š Document length: {len(full_text)} characters")
    
    # Find all occurrences of key terms
    key_terms = ['Key Rating Drivers', 'Strengths', 'Weakness', 'Liquidity', 'Outlook']
    
    print(f"\nğŸ“ Key term positions:")
    for term in key_terms:
        positions = []
        start = 0
        while True:
            pos = full_text.lower().find(term.lower(), start)
            if pos == -1:
                break
            positions.append(pos)
            start = pos + 1
        
        print(f"   {term}: {positions}")
        
        # Show context for first occurrence
        if positions:
            pos = positions[0]
            start = max(0, pos - 100)
            end = min(len(full_text), pos + len(term) + 200)
            context = full_text[start:end]
            print(f"      Context: ...{context}...")
    
    print(f"\nğŸ¯ Searching for section boundaries...")
    
    # Look for the specific structure
    rating_match = re.search(r'Key Rating Drivers.*?Detailed Description(.*?)(?=Liquidity|Outlook|Analytical)', 
                            full_text, re.DOTALL | re.IGNORECASE)
    
    if rating_match:
        section_content = rating_match.group(1)
        print(f"âœ… Found Key Rating section: {len(section_content)} characters")
        print(f"ğŸ“„ Preview:\n{section_content[:500]}...")
    else:
        print("âŒ Could not find Key Rating section with standard pattern")

def process_folder(folder_path):
    """Process all HTML files in a folder."""
    all_results = {}
    
    html_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.html', '.htm'))]
    print(f"ğŸ“ Found {len(html_files)} HTML files")
    
    for filename in html_files:
        file_path = os.path.join(folder_path, filename)
        try:
            print(f"\n{'='*80}")
            print(f"ğŸ”„ Processing: {filename}")
            print(f"{'='*80}")
            
            strengths, weaknesses = extract_strengths_weaknesses(file_path)
            
            file_key = filename.replace('.html', '').replace('.htm', '')
            all_results[file_key] = {
                'strengths': strengths,
                'weaknesses': weaknesses
            }
            
        except Exception as e:
            print(f"âŒ Error with {filename}: {e}")
            import traceback
            traceback.print_exc()
    
    return all_results

def save_results(results):
    """Save and display results."""
    with open('extracted_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*80}")
    print("ğŸ“Š FINAL RESULTS")
    print(f"{'='*80}")
    
    for filename, data in results.items():
        print(f"\nğŸ“„ {filename}:")
        print(f"   ğŸ“ˆ Strengths: {len(data['strengths'])}")
        print(f"   ğŸ“‰ Weaknesses: {len(data['weaknesses'])}")
        
        for title, desc in data['strengths'].items():
            print(f"      ğŸ’ª {title}")
        
        for title, desc in data['weaknesses'].items():
            print(f"      âš ï¸  {title}")
    
    print(f"\nğŸ’¾ Results saved to 'extracted_results.json'")

if __name__ == "__main__":
    folder_path = 'html_files'
    
    if not os.path.exists(folder_path):
        print(f"âŒ Folder '{folder_path}' not found!")
        exit()
    
    html_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.html', '.htm'))]
    if html_files:
        print("ğŸ” Running comprehensive debug on first file...")
        debug_html_structure(os.path.join(folder_path, html_files[0]))
        
        response = input(f"\nğŸ¤” Continue processing all {len(html_files)} files? (y/n): ").lower()
        if response != 'y':
            exit()
    
    results = process_folder(folder_path)
    if results:
        save_results(results)

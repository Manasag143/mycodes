from bs4 import BeautifulSoup
import re
import os
import json

def extract_strengths_weaknesses(html_file_path):
    """Extract strengths and weaknesses from a single HTML file."""
    with open(html_file_path, 'r', encoding='utf-8') as file:
        soup = BeautifulSoup(file.read(), 'html.parser')
    
    return extract_from_soup(soup)

def extract_from_html_content(html_content):
    """Extract strengths and weaknesses from HTML content string."""
    soup = BeautifulSoup(html_content, 'html.parser')
    return extract_from_soup(soup)

def is_bold_element(element):
    """Check if element has bold styling using various methods."""
    # Method 1: Check style attribute for font-weight
    style = element.get('style', '')
    if 'font-weight' in style and ('bold' in style or '700' in style or 'bolder' in style):
        return True
    
    # Method 2: Check for bold tags (strong, b)
    if element.name in ['strong', 'b']:
        return True
    
    # Method 3: Check class attribute
    classes = element.get('class', [])
    for cls in classes:
        if 'bold' in cls.lower():
            return True
    
    # Method 4: Check parent elements for bold styling
    parent = element.parent
    if parent:
        parent_style = parent.get('style', '')
        if 'font-weight' in parent_style and ('bold' in parent_style or '700' in parent_style):
            return True
    
    return False

def find_target_area(soup):
    """
    STEP 1: Find and extract the targeted area containing Key Rating Drivers
    Returns the BeautifulSoup element containing the target area
    """
    print("ğŸ¯ STEP 1: Finding targeted area...")
    
    target_section = None
    
    # Method 1: Find by "Key Rating Drivers" text
    for element in soup.find_all(['p', 'span', 'td', 'div']):
        if 'Key Rating Drivers' in element.get_text():
            # Try to find the appropriate parent container
            target_section = element.find_parent()
            
            # If the parent seems too small, try going up one more level
            if target_section and len(target_section.get_text()) < 500:
                larger_parent = target_section.find_parent()
                if larger_parent and 'Strengths' in larger_parent.get_text():
                    target_section = larger_parent
            
            break
    
    # Method 2: If not found, look for elements containing both "Strengths" and "Weakness"
    if not target_section:
        print("   ğŸ”„ Trying alternative method...")
        for element in soup.find_all(['td', 'div']):
            text = element.get_text()
            if ('Strengths' in text and 'Weakness' in text and 
                len(text) > 500):  # Must be substantial content
                target_section = element
                break
    
    if target_section:
        text_length = len(target_section.get_text())
        print(f"   âœ… Found target area: {text_length} characters")
        
        # Show preview of the target area
        preview_text = target_section.get_text()[:500]
        print(f"   ğŸ“„ Preview: {preview_text}...")
        
        return target_section
    else:
        print("   âŒ Could not find target area")
        return None

def extract_from_soup(soup):
    """
    Main extraction function using targeted area approach
    """
    print("ğŸš€ Starting extraction with targeted area approach...")
    
    # STEP 1: Find the target area
    target_section = find_target_area(soup)
    
    if not target_section:
        print("âŒ No target area found")
        return {}, {}
    
    # STEP 2: Apply the working extraction logic to the target area
    print("\nğŸ” STEP 2: Applying extraction logic to target area...")
    
    strengths_dict, weaknesses_dict = extract_from_target_section(target_section)
    
    print(f"\nğŸ“Š Final Results:")
    print(f"   ğŸ“ˆ Strengths: {len(strengths_dict)}")
    print(f"   ğŸ“‰ Weaknesses: {len(weaknesses_dict)}")
    
    return strengths_dict, weaknesses_dict

def extract_from_target_section(target_section):
    """
    Apply the original working logic to the targeted section
    This is the same logic from the working code, just applied to target area
    """
    strengths_dict = {}
    weaknesses_dict = {}
    current_section = None
    
    # Get all elements in the target section (paragraphs, lists, list items)
    all_elements = target_section.find_all(['p', 'ul', 'li', 'div'])
    
    print(f"   ğŸ” Processing {len(all_elements)} elements in target area...")
    
    for i, element in enumerate(all_elements):
        text = element.get_text().strip()
        
        # Skip empty elements
        if not text:
            continue
        
        # Debug: Show what we're processing
        if len(text) > 20:
            print(f"      Element {i}: {element.name} - {text[:60]}...")
        
        # Check if this is a section header (Strengths/Weaknesses)
        if element.name in ['p', 'div']:
            if re.search(r'\bStrengths?\s*:?\s*$', text, re.IGNORECASE):
                current_section = 'strengths'
                print(f"      ğŸ“ Found Strengths header at element {i}")
                continue
            elif re.search(r'\bWeakness(es)?\s*:?\s*$', text, re.IGNORECASE):
                current_section = 'weaknesses'
                print(f"      ğŸ“ Found Weaknesses header at element {i}")
                continue
        
        # Also check for headers that contain content (like "Strengths: content here")
        if current_section is None:
            if 'Strengths:' in text and len(text) > 20:
                current_section = 'strengths'
                print(f"      ğŸ“ Found Strengths section with content at element {i}")
                # Process this element as content too
            elif 'Weakness' in text and len(text) > 20:
                current_section = 'weaknesses'
                print(f"      ğŸ“ Found Weaknesses section with content at element {i}")
                # Process this element as content too
        
        # Process list items within a section
        if element.name == 'li' and current_section:
            key, value = extract_key_value_from_element(element)
            if key:
                if current_section == 'strengths':
                    strengths_dict[key] = value
                    print(f"      âœ… Strength: {key[:50]}...")
                elif current_section == 'weaknesses':
                    weaknesses_dict[key] = value
                    print(f"      âœ… Weakness: {key[:50]}...")
        
        # Also handle paragraphs and divs with bold content (alternative structure)
        elif element.name in ['p', 'div'] and current_section and len(text) > 30:
            key, value = extract_key_value_from_element(element)
            if key:
                if current_section == 'strengths':
                    strengths_dict[key] = value
                    print(f"      âœ… Strength: {key[:50]}...")
                elif current_section == 'weaknesses':
                    weaknesses_dict[key] = value
                    print(f"      âœ… Weakness: {key[:50]}...")
    
    return strengths_dict, weaknesses_dict

def extract_key_value_from_element(element):
    """
    Extract key-value pair from a single element
    This uses the same logic as the working code
    """
    # Look for bold elements within this element
    bold_elements = []
    
    # Find all spans with bold styling
    spans = element.find_all('span')
    for span in spans:
        if is_bold_element(span):
            bold_elements.append(span)
    
    # Also check for strong/b tags
    bold_tags = element.find_all(['strong', 'b'])
    bold_elements.extend(bold_tags)
    
    if bold_elements:
        # Get the first bold element as the key
        key_element = bold_elements[0]
        key = key_element.get_text().strip().rstrip(':')
        
        # Get the full text and remove the key part to get the value
        full_text = element.get_text().strip()
        key_text = key_element.get_text().strip()
        
        # Remove the key from the full text to get the value
        if key_text in full_text:
            value = full_text.replace(key_text, '', 1).strip()
            # Remove leading colon if present
            value = value.lstrip(':').strip()
        else:
            value = full_text
        
        # Validate the key (must be meaningful)
        if key and len(key) > 5 and len(key) < 300:
            # Clean up the key and value
            key = clean_text(key)
            value = clean_text(value)
            return key, value
    
    # If no bold elements found, try colon-based extraction
    full_text = element.get_text().strip()
    if ':' in full_text and len(full_text) > 30:
        colon_pos = full_text.find(':')
        potential_key = full_text[:colon_pos].strip()
        potential_value = full_text[colon_pos+1:].strip()
        
        # Validate the potential key
        if (len(potential_key) > 5 and len(potential_key) < 300 and
            not potential_key.lower().startswith(('the ', 'this ', 'it '))):
            return clean_text(potential_key), clean_text(potential_value)
    
    return None, None

def clean_text(text):
    """Clean and normalize text content."""
    if not text:
        return ""
    
    # Remove extra whitespace and normalize
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    # Remove leading/trailing colons
    text = text.strip(':').strip()
    
    # Remove section headers that might have leaked in
    text = re.sub(r'^(Strengths?|Weakness(?:es)?)\s*:?\s*', '', text, flags=re.IGNORECASE)
    
    return text

def debug_html_structure(html_file_path):
    """Enhanced debug function for targeted area approach."""
    with open(html_file_path, 'r', encoding='utf-8') as file:
        soup = BeautifulSoup(file.read(), 'html.parser')
    
    print("ğŸ” DEBUGGING WITH TARGETED AREA APPROACH")
    print("=" * 60)
    
    # Step 1: Show target area finding
    target_section = find_target_area(soup)
    
    if target_section:
        print(f"\nâœ… Target area found!")
        
        # Show structure of target area
        all_elements = target_section.find_all(['p', 'ul', 'li', 'div'])
        print(f"ğŸ“Š Target area contains {len(all_elements)} elements")
        
        # Show first few elements
        print(f"\nğŸ“‹ First 10 elements in target area:")
        for i, element in enumerate(all_elements[:10]):
            text = element.get_text().strip()
            if text:
                print(f"   {i+1}. {element.name}: {text[:80]}...")
                
                # Check for bold content
                bold_spans = element.find_all('span')
                bold_content = []
                for span in bold_spans:
                    if is_bold_element(span):
                        bold_content.append(span.get_text().strip())
                
                strong_tags = [tag.get_text().strip() for tag in element.find_all(['strong', 'b'])]
                
                if bold_content or strong_tags:
                    print(f"      ğŸ”¥ Bold content: {bold_content + strong_tags}")
        
        # Check for section keywords
        target_text = target_section.get_text()
        strengths_count = target_text.lower().count('strengths')
        weakness_count = target_text.lower().count('weakness')
        
        print(f"\nğŸ“Š Section analysis:")
        print(f"   'Strengths' appears: {strengths_count} times")
        print(f"   'Weakness' appears: {weakness_count} times")
        
    else:
        print("âŒ No target area found!")
        
        # Show what we can find in the whole document
        full_text = soup.get_text()
        print(f"ğŸ“Š Full document length: {len(full_text)} characters")
        
        if 'Key Rating Drivers' in full_text:
            print("âœ… 'Key Rating Drivers' found in document")
        if 'Strengths' in full_text:
            print("âœ… 'Strengths' found in document")
        if 'Weakness' in full_text:
            print("âœ… 'Weakness' found in document")

def process_folder(folder_path):
    """Process all HTML files in a folder."""
    all_results = {}
    
    # Find all HTML files
    html_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.html', '.htm'))]
    
    print(f"ğŸ“ Found {len(html_files)} HTML files")
    
    # Process each file
    for filename in html_files:
        file_path = os.path.join(folder_path, filename)
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ”„ Processing: {filename}")
            print(f"{'='*60}")
            
            strengths, weaknesses = extract_strengths_weaknesses(file_path)
            
            file_key = filename.replace('.html', '').replace('.htm', '')
            all_results[file_key] = {
                'strengths': strengths,
                'weaknesses': weaknesses
            }
            
            print(f"âœ… Completed {filename}: {len(strengths)} strengths, {len(weaknesses)} weaknesses")
            
        except Exception as e:
            print(f"âŒ Error with {filename}: {e}")
            import traceback
            traceback.print_exc()
    
    return all_results

def save_and_print_results(results):
    """Save to JSON and print summary."""
    # Clean results to remove empty keys
    cleaned_results = {}
    
    for filename, data in results.items():
        cleaned_strengths = {}
        cleaned_weaknesses = {}
        
        # Clean strengths
        for key, value in data['strengths'].items():
            key = clean_text(key)
            value = clean_text(value)
            if key and len(key) > 3:  # Only keep meaningful keys
                cleaned_strengths[key] = value
        
        # Clean weaknesses
        for key, value in data['weaknesses'].items():
            key = clean_text(key)
            value = clean_text(value)
            if key and len(key) > 3:  # Only keep meaningful keys
                cleaned_weaknesses[key] = value
        
        cleaned_results[filename] = {
            'strengths': cleaned_strengths,
            'weaknesses': cleaned_weaknesses
        }
    
    # Save to JSON
    with open('extracted_results.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned_results, f, indent=2, ensure_ascii=False)
    
    # Print summary
    print(f"\n{'='*80}")
    print("ğŸ“Š FINAL RESULTS SUMMARY")
    print(f"{'='*80}")
    
    for filename, data in cleaned_results.items():
        print(f"\nğŸ“„ {filename}:")
        print(f"   ğŸ“ˆ Strengths: {len(data['strengths'])}")
        print(f"   ğŸ“‰ Weaknesses: {len(data['weaknesses'])}")
        
        # Print actual content
        if data['strengths']:
            print("\n   ğŸ”¹ STRENGTHS:")
            for i, (key, value) in enumerate(data['strengths'].items(), 1):
                print(f"      {i}. {key}")
                if value:
                    print(f"         â†’ {value[:100]}{'...' if len(value) > 100 else ''}")
                print()
        
        if data['weaknesses']:
            print("\n   ğŸ”¸ WEAKNESSES:")
            for i, (key, value) in enumerate(data['weaknesses'].items(), 1):
                print(f"      {i}. {key}")
                if value:
                    print(f"         â†’ {value[:100]}{'...' if len(value) > 100 else ''}")
                print()
    
    print(f"\nğŸ’¾ Results saved to 'extracted_results.json'")

if __name__ == "__main__":
    folder_path = 'html_files'
    
    # Check if folder exists
    if not os.path.exists(folder_path):
        print(f"âŒ Folder '{folder_path}' not found!")
        exit()
    
    # Debug first file
    html_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.html', '.htm'))]
    if html_files:
        print("ğŸ” Debugging first HTML file with targeted area approach...")
        debug_html_structure(os.path.join(folder_path, html_files[0]))
        print("\n" + "="*80)
        
        response = input(f"\nğŸ¤” Continue processing all {len(html_files)} files? (y/n): ").lower()
        if response != 'y':
            print("ğŸ‘‹ Exiting...")
            exit()
    
    # Process all files
    print("\nğŸš€ Processing all HTML files with targeted area approach...")
    results = process_folder(folder_path)
    
    if results:
        save_and_print_results(results)
    else:
        print("âŒ No HTML files found or processed successfully!")
